{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "from collections import defaultdict\n",
    "from glob import glob\n",
    "from random import choice, sample\n",
    "import os.path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from tensorflow.keras.utils import load_img"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from keras_vggface.vggface import VGGFace\n",
    "from keras_vggface.utils import preprocess_input\n",
    "\n",
    "from keras.layers import Input, Dense, GlobalMaxPool2D, GlobalAvgPool2D, Concatenate, Multiply, Dropout, Subtract\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau"
   ],
   "id": "f9280305b47c60ab",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ],
   "id": "1df0283df02139c4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Exploratory Data Analysis\n",
    "\n",
    "Understand the images, the directory structure, and references/keys between people/relationships/images work. \n",
    "\n",
    "`F*` top-level directories contain families.\n",
    "\n",
    "`MID*` directories inside them contain members: mothers/fathers/children.\n",
    "\n",
    "`train_relationships.csv` contains positive examples with `parent-sibiling` (e.g.: `mother-child` and `father-child`) relationships."
   ],
   "id": "907dc4d68c71f0ff"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "train_relationships = pd.read_csv('data/train_relationships.csv')\n",
    "train_relationships"
   ],
   "id": "411d64ee6f1d8470",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "train_images = [os.path.join(root, file).replace('\\\\', '/') for root, _, files in os.walk(os.path.expanduser(\"data/train\")) for file in files]\n",
    "train_images"
   ],
   "id": "9a003f2672627e7d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "all_images_df = pd.DataFrame({\n",
    "    'files': train_images,\n",
    "    'familyId': [file.split('/')[3] for file in train_images],\n",
    "    'kinId': [file.split('/')[4] for file in train_images],\n",
    "    'uniqueId': [file.split('/')[3] + '/' + file.split('/')[4] for file in train_images]\n",
    "})\n",
    "all_images_df"
   ],
   "id": "6553e64e63f3c151",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "families = all_images_df[\"familyId\"].unique()\n",
    "pd.DataFrame(families)"
   ],
   "id": "726f5072a279e1f5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "distinct_image_sizes = {Image.open(path).size for path in all_images_df.files}\n",
    "assert(len(distinct_image_sizes) == 1)\n",
    "\n",
    "# all images have the same dimension\n",
    "# no need for resizing\n",
    "\n",
    "distinct_image_sizes"
   ],
   "id": "9460a64d70ae8f99",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Preprocess",
   "id": "e25eb31688fa300"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Select F09* families for validation, the rest for training\n",
    "val_families = \"F09\"\n",
    "\n",
    "all_images = glob(\"data/train/*/*/*.jpg\")\n",
    "all_images = [p.replace('\\\\', '/') for p in all_images]\n",
    "train_images = [p for p in all_images if val_families not in p]\n",
    "validation_images = [p for p in all_images if val_families in p]\n",
    "\n",
    "assert(len(all_images) == len(train_images) + len(validation_images))\n",
    "\n",
    "# split 90.7% + 9.3%, test set provided by Kaggle\n",
    "print(f'all        images: {len(all_images)}')\n",
    "print(f'train      images: {len(train_images)}')\n",
    "print(f'validation images: {len(validation_images)}')"
   ],
   "id": "74b393ea3f624fdd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# training paths - to images not belonging to `F09` families\n",
    "train_person_to_images_map = defaultdict(list)\n",
    "for x in train_images:\n",
    "    train_person_to_images_map[x.split(\"/\")[-3] + \"/\" + x.split(\"/\")[-2]].append(x)\n",
    "\n",
    "# validation paths - to images belonging to `F09` families\n",
    "val_person_to_images_map = defaultdict(list)\n",
    "for x in validation_images:\n",
    "    val_person_to_images_map[x.split(\"/\")[-3] + \"/\" + x.split(\"/\")[-2]].append(x)"
   ],
   "id": "6ab4ad60e1b17e15",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# must not contain paths starting with `data/train/F09`\n",
    "train_person_to_images_map"
   ],
   "id": "e887026576c05b80",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# must only contain paths starting with `data/train/F09`\n",
    "validation_images"
   ],
   "id": "63c5a347b0003765",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# people found in train.zip\n",
    "ppl = [x.split(\"/\")[-3] + \"/\" + x.split(\"/\")[-2] for x in all_images]\n",
    "\n",
    "# filter out people not found in train.zip but present in train_relationships.csv\n",
    "# to make all data cross-referencing;\n",
    "# similar to an INNER JOIN\n",
    "relationships = list(zip(train_relationships.p1.values, train_relationships.p2.values))\n",
    "relationships = [x for x in relationships if x[0] in ppl and x[1] in ppl]\n",
    "\n",
    "train = [x for x in relationships if val_families not in x[0]]\n",
    "val = [x for x in relationships if val_families in x[0]]\n",
    "\n",
    "print(f'train relationships:      {len(train)}')\n",
    "print(f'validation relationships: {len(val)}')"
   ],
   "id": "3f7cd39e1c04da17",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# reorder channels from RGB to BGR\n",
    "# zero-center all channels with respect to ImageNet\n",
    "# no scaling, no actual normalization\n",
    "def normalize_image(path):\n",
    "    img = load_img(path)\n",
    "    img = np.array(img).astype(np.float)\n",
    "    return preprocess_input(img)\n",
    "\n",
    "def generator(list_tuples, person_to_images_map, batch_size=16):\n",
    "    ppl = list(person_to_images_map.keys())\n",
    "    while True:\n",
    "        batch_tuples = sample(list_tuples, batch_size // 2)\n",
    "        labels = [1] * len(batch_tuples)\n",
    "\n",
    "        while len(batch_tuples) < batch_size:\n",
    "            p1 = choice(ppl)\n",
    "            p2 = choice(ppl)\n",
    "\n",
    "            if p1 != p2 and (p1, p2) not in list_tuples and (p2, p1) not in list_tuples:\n",
    "                batch_tuples.append((p1, p2))\n",
    "                labels.append(0)\n",
    "\n",
    "        for x in batch_tuples:\n",
    "            if not len(person_to_images_map[x[0]]):\n",
    "                print(x[0])\n",
    "\n",
    "        X1 = [choice(person_to_images_map[x[0]]) for x in batch_tuples]\n",
    "        X1 = np.array([normalize_image(x) for x in X1])\n",
    "\n",
    "        X2 = [choice(person_to_images_map[x[1]]) for x in batch_tuples]\n",
    "        X2 = np.array([normalize_image(x) for x in X2])\n",
    "        labels = np.asarray(labels).astype('float32').reshape((-1,1))\n",
    "\n",
    "        yield [X1, X2], labels"
   ],
   "id": "4d89d0af819ca858",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def build_model():\n",
    "    # images are 3-channel 224x224\n",
    "    input_1 = Input(shape=(224, 224, 3))\n",
    "    input_2 = Input(shape=(224, 224, 3))\n",
    "\n",
    "    # VGGFace pre-trained on faces\n",
    "    # only load feature extraction layers\n",
    "    base_model = VGGFace(model='resnet50', include_top=False)\n",
    "    \n",
    "    # senet50 expands resnet50 with an added Squeeze-and-Excitation block \n",
    "    # base_model = VGGFace(model='senet50', include_top=False)\n",
    "\n",
    "    # transfer learning - feature extraction\n",
    "    # use a pre-trained model as a fixed feature extractor\n",
    "    # train its final 3 layers for our application, freeze the rest\n",
    "    for x in base_model.layers[:-3]:\n",
    "        x.trainable = True\n",
    "\n",
    "    x1 = base_model(input_1)\n",
    "    x2 = base_model(input_2)\n",
    "\n",
    "    x1 = Concatenate(axis=-1)([GlobalMaxPool2D()(x1), GlobalAvgPool2D()(x1)])\n",
    "    x2 = Concatenate(axis=-1)([GlobalMaxPool2D()(x2), GlobalAvgPool2D()(x2)])\n",
    "\n",
    "    # Distance metric: (x1 - x2)^2 + x1^2 * x2^2 + x1 * x2\n",
    "    x3 = Subtract()([x1, x2])\n",
    "    x3 = Multiply()([x3, x3])\n",
    "\n",
    "    x1_ = Multiply()([x1, x1])\n",
    "    x2_ = Multiply()([x2, x2])\n",
    "    x4 = Subtract()([x1_, x2_])\n",
    "\n",
    "    x5 = Multiply()([x1, x2])\n",
    "\n",
    "    x = Concatenate(axis=-1)([x4, x3, x5])\n",
    "\n",
    "    # hidden layers activation: ReLu\n",
    "    x = Dense(100, activation=\"relu\")(x)\n",
    "    \n",
    "    x = Dropout(0.01)(x)\n",
    "    \n",
    "    # output layer activation: Sigmoid\n",
    "    out = Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "    model = Model([input_1, input_2], out)\n",
    "    # loss function: Binary Cross-Entropy\n",
    "    model.compile(loss=\"binary_crossentropy\", metrics=['accuracy'], optimizer=Adam(0.00001))\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    return model"
   ],
   "id": "3b874b846df9a5fe",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "file_path = \"faces.h5\"\n",
    "\n",
    "# Save the Keras model/weights frequently\n",
    "checkpoint = ModelCheckpoint(\n",
    "    file_path,\n",
    "    monitor='val_accuracy', # best validation accuracy\n",
    "    verbose=1,\n",
    "    save_best_only=True,\n",
    "    mode='max')\n",
    "\n",
    "# Reduce learning rate when the validation accuracy stopped improving\n",
    "reduce_on_plateau = ReduceLROnPlateau(\n",
    "    monitor=\"val_accuracy\",\n",
    "    mode=\"max\",\n",
    "    factor=0.1,\n",
    "    patience=20,\n",
    "    verbose=1)\n",
    "\n",
    "callbacks_list = [checkpoint, reduce_on_plateau]\n",
    "\n",
    "model = build_model()\n",
    "\n",
    "if os.path.isfile(file_path):\n",
    "    model.load_weights(file_path)\n",
    "\n",
    "hist = model.fit(\n",
    "    generator(train, train_person_to_images_map, batch_size=16),\n",
    "    use_multiprocessing=False,\n",
    "    validation_data=generator(val, val_person_to_images_map, batch_size=16),\n",
    "    epochs=30,\n",
    "    verbose=2,\n",
    "    workers=1,\n",
    "    callbacks=callbacks_list,\n",
    "    steps_per_epoch=50,\n",
    "    validation_steps=50)"
   ],
   "id": "d91699809706fa6a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Testing",
   "id": "fc834e96e83bb754"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def batcher(seq, size=32):\n",
    "    return (seq[pos:pos + size] for pos in range(0, len(seq), size))\n",
    "\n",
    "test_path = \"data/test/\"\n",
    "\n",
    "submission = pd.read_csv('data/sample_submission.csv')\n",
    "\n",
    "predictions = []\n",
    "\n",
    "# \"taqaddum\" progress bars\n",
    "for batch in tqdm(batcher(submission.img_pair.values)):\n",
    "    X1 = [x.split(\"-\")[0] for x in batch]\n",
    "    X1 = np.array([normalize_image(test_path + x) for x in X1])\n",
    "\n",
    "    X2 = [x.split(\"-\")[1] for x in batch]\n",
    "    X2 = np.array([normalize_image(test_path + x) for x in X2])\n",
    "\n",
    "    # https://numpy.org/doc/stable/reference/generated/numpy.ravel.html\n",
    "    pred = model.predict([X1, X2]).ravel().tolist()\n",
    "    predictions += pred\n",
    "\n",
    "submission['is_related'] = predictions\n",
    "\n",
    "submission.to_csv(\"face.csv\", index=False)"
   ],
   "id": "af4be9e5824adefd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Plot",
   "id": "6a2b28656f8d54a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(20, 6))\n",
    "\n",
    "# loss to the left\n",
    "axes[0].plot(hist.history['loss'], label='loss')\n",
    "axes[0].plot(hist.history['val_loss'], label='val_loss')\n",
    "axes[0].legend(prop={'size':15})\n",
    "\n",
    "# accuracy to the right\n",
    "axes[1].plot(hist.history['accuracy'], label='acc')\n",
    "axes[1].plot(hist.history['val_accuracy'], label='val_acc')\n",
    "axes[1].legend(prop={'size':15})\n",
    "\n",
    "plt.savefig('faces.png')"
   ],
   "id": "7120887be26501da",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
